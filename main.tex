\documentclass[11pt]{article}

%  USE PACKAGES  ---------------------- 
\usepackage[margin=0.75in,vmargin=1in]{geometry}
\usepackage{amsmath,amsthm,amsfonts}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{mathtools}
\usepackage{hyperref,color}
\usepackage{enumitem,amssymb}
\usepackage{graphicx}
\newcommand\tab[1][1cm]{\hspace*{#1}}
\newlist{todolist}{itemize}{4}
\setlist[todolist]{label=$\square$}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\done}{\rlap{$\square$}{\raisebox{2pt}{\large\hspace{1pt}\cmark}}%
\hspace{-2.5pt}}
\newcommand{\HREF}[2]{\href{#1}{#2}}
\usepackage{textcomp}
\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
% columns=flexible,
upquote=true,
breaklines=true,
showstringspaces=false
}
\usepackage{longtable}
\usepackage{subcaption}
\usepackage{blindtext}
\usepackage{titlesec}
%  -------------------------------------------- 

%  HEADER AND FOOTER (DO NOT EDIT) ----------------------
\newcommand{\problemnumber}{0}
\pagestyle{fancy}
\fancyhead{}
% \fancyhead[L]{\textbf{\problemnumber}}
\newcommand{\newquestion}[1]{
\clearpage % page break and flush floats
\renewcommand{\problemnumber}{#1} % set problem number for header
\phantom{}  % Put something on the page so it shows
}
\fancyfoot[L]{IE 332}
\fancyfoot[C]{Project submission}
\fancyfoot[R]{Page \thepage}
\renewcommand{\footrulewidth}{0.4pt}

%  --------------------------------------------


%  COVER SHEET (FILL IN THE TABLE AS INSTRUCTED IN THE ASSIGNMENT) ----------------------
\newcommand{\addcoversheet}{
\clearpage
\thispagestyle{empty}
\vspace*{0.5in}

\begin{center}
\Huge{{\bf IE332 Project \#2}} % <-- replace with correct assignment #

Due: April 28th, 11:59pm EST % <-- replace with correct due date and time
\end{center}

\vspace{0.3in}

\noindent We have {\bf read and understood the assignment instructions}. We certify that the submitted work does not violate any academic misconduct rules, and that it is solely our own work. By listing our names below we acknowledge that any misconduct will result in appropriate consequences. 

\vspace{0.2in}

\noindent {\em ``As a Boilermaker pursuing academic excellence, I pledge to be honest and true in all that I do.
Accountable together -- we are Purdue.''}

\vspace{0.3in}

\begin{table}[h!]
  \begin{center}
    \label{tab:table1}
    \begin{tabular}{c|ccccc|c|c}
      Student & Q1 & Q2 & Q3 & Q4 & Q5 & Overall & DIFF\\
      \hline
      Taylor Kalata & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
      Alex Hart & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
      Grant Laneve & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
      Angelica Jovceski & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
      Sam Preston & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
      \hline
      St Dev & 0 & 0 & 0 & 0 & 0 & 0 & 0
    \end{tabular}
  \end{center}
\end{table}

\vspace{0.2in}

\noindent Date: \today.
}


%  -----------------------------------------

\begin{document}

\addcoversheet
\newpage
\tableofcontents


% BEGIN YOUR ASSIGNMENT HERE:
\newpage
\section{Introduction}
%Overview of the project and the required algorithms
%angelica

Machine learning enables the development of complex models to learn from data and perform various tasks such as image classification. Image classification is where a model learns to recognize and categorize images into different classes. Although, with any technology, machine learning algorithms are not immune to attacks.

Adversarial attacks have been developed to fool machine learning models by making small, changes to the input data, which cause the model to incorrectly classify the image. Adversarial attacks can be used to test the robustness of machine learning models and can also be used to deceive and manipulate models in the real world. 

In this project, we aimed to train a voting-based optimization algorithm to perform adversarial attacks on a binary image classifier. We used five different machine learning algorithms to fool the classifier, and placed them within another algorithm that assigns weights to them based on their expected performance given the image.


The ultimate goal of this project was to successfully fool the provided image classifier with a given pixel budget and to do so in less than 10 seconds per image. We evaluated our models on a set of reserved 100\% accurate data and scored them based on their successful fooled images at different budget levels. Our report will detail the rationale for selecting each of the five machine learning algorithms, how we trained them, how our optimizer allocates the votes, and why such an approach should work. We will also provide an analysis of the performance of our algorithms, their accuracy, runtime complexity, and average wall-time, as well as the lessons learned and future directions for research in this area.





\section{Purpose of Image Classifiers}
%applications of image classifiers and why they benefit different industries
%alex
In the age of Artificial Intelligence and the Internet of Things, image classifiers play an important role in providing analysis for the massive amounts of unstructured image data we obtain. Image classification is the basis for computer vision: a field in which the goal is to enable a computer to see and understand objects in photos and videos. In some cases of classification, computers access massive amounts of data and can sense details leading to a high level of accuracy in classification that can far exceed that of a human (“What is …”, 2023). 
While in this project the model is distinguishing between only grass and dandelions with a given data set, search engines such as Google are given less constraints and more data to decipher in order to predict images based on random keywords or identify objects in a reverse image search. Visual search engines are one of many applications of image classifiers. Some other prevalent examples include facial recognition, traffic control systems, and self-driving cars (Boesch, 2023).


\section{Danger of Adversary Attacks}
%explain why these attacks can be dangerous
%taylor
As mentioned above, image classifiers can be a helpful technological advance in a variety of industries. However, these classifier models can be very susceptible to adversarial attacks due to their brittle nature (PureAI Editors, 2021). Slightly changing pixels of images can completely change the model result. A simple binary image classifier to detect a dandelion will probably not be a target of an attack in the real world. But when we look at industries like medical, military, financial, security, etc, the effects of a faulty model can have serious consequences. Not only can these attacks cause incorrect classification but also doing so with reported high confidence in its prediction. For example, adversary attacks can be very dangerous with the emerging presence of self-driving cars. The "sight" of the car is made possible through deep neural networks that make up the model to classify different objects the car encounters. When the model is under attack, it can cause images to be classified incorrectly leading to accidents and injury. The self-driving car may identify a stop sign as a speed limit sign; graffiti could be identified as a person or animal in the road; another car could be mistaken for an open road. These are just some of many ways an adversary attack could potentially harm humans. It can especially be dangerous due to the fact the humans do not see any potential threat. If people are not aware of these possible attacks and their implications, it is easy to trust the models and not think twice about incorrect classifications. 
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.75\textwidth, height=5cm]{car_accident.png}
\caption{Adversarial Attack Effect}
\label{fig:figure2}
\end{center}
\end{figure}

Because of the potential danger in many industries, defense against these attacks is being researched heavily (Wiggers, 2021). One way involves modifying a model to respond to input triggers that are used in these attacks. Researchers are implementing a Trojan horse method to test the effectiveness of their defense tactics. This allows them to better understand how the model reacts to attacks and what modifications will benefit the robustness of the neural networks. This method provides a basis for further research and techniques to boost model security. Increasing the reliability of these image classifiers will allow different fields to safely implement them to elevate the ability of machines in industry practice.

\section{Algorithm explanations}
%explain each algorithm goal/how it works
%why you picked it (fast time, accuracy, etc)
% loop invarient & runtime complexity -> IN APPENDIX, NOT THIS SECTION


% selecting each of the 5 machine learning algorithms: how you trained them, and how your optimizer allocates the votes, and why such an approach should work

% Angelica - Deep Fool

%Alex - FGSM

%Taylor - BIM
\tab Another algorithm we researched was the Basic Iterative Method (BIM). This is an extension of the Fast Gradient Sign Method (FGSM) explained above (NueralCeption, 2023). This algorithm applies FGSM multiple times to an image with a specified step size $\alpha$, where $\alpha$ is chosen to be the intensity of one pixel. Another parameter of BIM is $\epsilon$, which limits the adversarial pertubance. This means that a maximum is set to the amount of change each selected pixel endures. The number of iterations depends on the maximum perturbance $\epsilon$ and the step size $\alpha$ to reach the maximum. This iterative approach changes the image pixels every time the FGSM algorithm is run in opposition of the orginal FGSM single large step. The attack function is passed the mean of the data, the standard deviation of the data, the model to attack, the related images, the labels from the images, epsilon, alpha, and the number of iterations. The function starts by converting the image labels to a multi-dimensional matrix of a consistent data type. We then initialize the attacked image as the orginal image that has been passed to the function input. The algorithm then normalizes epsilon, alpha, and the range based on mean and standard deviation of the function. The normalization converts the inputs to tensors, which are the multidimensional matrices that is the data type needed to perform operations with the images to be attacked. Once the maximum change in pixel value is calculated using $\epsilon$, the iterations of FGSM can begin over the specified number of iterations (Ansah, 2023). Each iteration of the loop with calculate the gradient with respect to the image pixels. Then it will slightly adjust the pixels toward the gradient to maximize loss between the original and attacked image. Again, this done over again for each iteration by the step size $\alpha$. Because the FGSM process is repeated many times, the BIM algorithm is going to perform slower. However, this method tends to be more successful in tricking neural network models. It also can provide more subtle changes to the images to go unnoticed by the human eye. Section 5 of our report dives further into the efficiency trade-off dilemma experienced here.


%Grant - Particle Swarm

%Sam - Simulated Annealing 

%%%%% what we did instead %%%%%

%Taylor - blur and color change "algorithm"
\tab Because of the course load and time constraints of this project, we were not able to fully implement the adversary attack algorithms that are explained above. In addition to these algorithms, we researched R functions to change the appearance of images in order to fool the given model. We were able to isotropically blur the images with a certain standard deviation of the blur. As the image is blurred, the model is not always able to tell the contrast between the dandelion and surrounding grass, causing it to incorrectly classify the image. We also implemented a function to alter the image color channels. Since the grass and dandelion pictures obviously have a distinction in having the color yellow or not, changing color channels can aid in fooling the given model. By setting the green and blue channels to zero, the red channel is left. The attacked images then lack the zeroed channels, and the model has to focus on other image characteristics rather than the obvious color differences. Performance and effects can be found in the report Appendix.


%Angelica - grayscale

%need to ask Lucas how to interpret the results of running the test loop
%occlude function does not trick or lower accuracy as of now
%blurry function does not trick but lowers accuracy
%add noise function does not trick but lowers accuracy (have ti add isoblur to it though)
%color function drastically lowers accuracy percentage for dandelion pictures but not grass

\section{Efficiency trade-off}
%general discussion of efficiency vs quality inverse relationship
A Central Processing Unit (CPU) has a limit on how many calculations it is able to perform in a certain time frame. Therefore, the speed of every computer is limited and the runtime of each algorithm should be considered. The more operations that occur, the longer it will take for the algorithm to execute. If an algorithm is higher quality, it typically will be more complex and will therefore take more resources. However, if an algorithm is less complex, it typically will require fewer steps and take less time. In other words, an efficient algorithm may be less accurate and an accurate algorithm may be less efficient.
However, this may not be the case in all instances, such as in machine learning. In these cases, if one were to over train an algorithm on a training data set, the process of creating the model would be less efficient as training takes time and resources. In addition to that, the model would be of lower quality as it would be tailored too much on training data and would struggle to model testing data.


    
\newpage
\section{References}
\begin{verbatim}
Editors01/05/2021, P. A. I. (n.d.). Defending machine learning image classification modelsfrom attacks. Pure AI. Retrieved April 23, 2023, from 
https://pureai.com/articles/2021/01/05/defending-model-attacks.aspx#:~:text=However%2C%20it%20has%20been%20known%20for%20several%20years,the%20image%20is%20completely%20misclassified%20by%20the%20model.   

Wiggers, K. (2021, May 29). Adversarial attacks in machine learning: What they are and how to stop them. VentureBeat. Retrieved April 23, 2023, from https://venturebeat.com/security/adversarial-attacks-in-machine-learning-what-they-are-and-how-to-stop-them/ 

Boesch, G. (2023, February 24). A complete guide to Image Classification in 2023. viso.ai. Retrieved April 24, 2023, from https://viso.ai/computer-vision/image-classification/#:~:text=Image%20Classification%20is%20the%20Basis%20of%20Computer%20Vision,-The%20field%20of&text=It%20forms%20the%20basis%20for,%2C%20machine%20vision%2C%20and%20more. 

What is Computer Vision? Microsoft Azure. (2023). Retrieved April 24, 2023, from https://azure.microsoft.com/en-us/resources/cloud-computing-dictionary/what-is-computer-vision/#:~:text=Computer%20vision%20is%20a%20field,tasks%20that%20replicate%20human%20capabilities. 

Basic iterative method. NeuralCeption. (2023, April 25). Retrieved April 25, 2023, from https://www.neuralception.com/adversarialexamples-bim 

Ansah, H. (2023, April 21). Adversarial attacks on neural networks: Exploring the fast gradient sign method. neptune.ai. Retrieved April 25, 2023, from https://neptune.ai/blog/adversarial-attacks-on-neural-networks-exploring-the-fast-gradient-sign-method 
\end{verbatim}

\begin{thebibliography}{4}

\end{thebibliography}

\section{Appendix}
% Per the document: include Testing/Correctness/Verification $(1 section), Runtime Complexity and Walltime (1 section), Performance (according to the aforementioned criterion), and justification for the algorithm selected in your final implementation out of the set of implementations you tested

\subsection{Testing/Correctness/Verification}
%correctness proofs of loops from psuedocode found online (you can include the pictures)



% Angelica - Deep Fool

%Alex - FGSM

%Taylor - BIM

%Grant - Particle Swarm

%Sam - Simulated Annealing 


\subsection{Runtime complexity/Walltime}
%explanation of how to calculate runtime
%runtime for each algorithm
%walltime is actual time it takes to run, explain difference
%sam
To calculate runtime for an algorithm, we first identify the number of operations for each iteration of the algorithm. These operations can be arithmetic operations, comparisons, and assignment operations. We represent these as weights. Next, we determine a mathematical expression in relation to the input size, n.
We then combine the weights, or number of operations, and multiply that by the mathematical expression that represents the time complexity.
\newline \newline Walltime is a term that represents the real-time of an algorithm, in a linear sense. Start to stop, for simpler terms. Runtime, on the other hand, refers to the time it takes to execute the code or instructions, excluding the idle and/or waiting time. 

% Angelica - Deep Fool

%Alex - FGSM

%Taylor - BIM


%Grant - Particle Swarm

%Sam - Simulated Annealing 

\subsection{Performance}
%before and after pictures

%Taylor - outcome of testing blurry and color

%Angelica - outcome of testing grayscale


\subsection{Combining Weights}

%what the weightings would be when combined into 1 algorithm


%\begin{figure}[h]
%\centering
%\subfigure[Figure 1: Label 1]{\includegraphics[width=0.4\linewidth]{picture_1.png}}
%\subfigure[Figure 2: Label 2]{\includegraphics[width=0.4\linewidth]{picture_2.png}}
%\end{figure}

\end{document}



%\https://www.sciencedirect.com/science/article/pii/S1077314298906750